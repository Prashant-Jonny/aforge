#summary Some ideas about Hands Gesture Recognition in still image and video feeds using AForge.NET framework.
#labels Project-Article

[http://aforge.googlecode.com/svn/wiki/images/hgr/Gestures_Recognition.jpg]

= Introduction =

Since the time I’ve wrote my first article about motion detection, I’ve got a lot of e-mails from different people around the world, which found the article quite useful and found a lot of applications of the code in many different areas. Those areas were including from simple video surveillance topics to quite impressing applications, like laser gestures recognition, detecting comets with telescope, detecting humming-birds and making camera shots of them, controlling water cannon and many other applications.

In this article I would like to discuss one more application, which uses motion detection as its first step and then does some interesting routines with the detected object – hands gesture recognition. Let’s suppose we have a camera, which monitors some area. When somebody gets into the area and makes some hands gestures in front of the camera, application should detect type of the gesture and raise an event, for example. When the hands gesture recognition is detected, the application may perform different actions depending on the type of gesture. For example, gestures recognition application may control some sort of device or another application sending different commands to it depending on the recognized gesture. What type of hands gestures are talking about? This particular application, which is discussed in the article, may recognize up to 12 gestures, which are combination of 4 different positions of 2 hands – hand is not raised, raised diagonally down, diagonally up or raised straight.

All the algorithms described in the article are based on the [http://code.google.com/p/aforge/ AForge.NET framework], which provides different image processing routines used by the application. The application also uses some motion detection routines, which are inspired by the framework and another article dedicated to [http://www.codeproject.com/KB/audio-video/Motion_Detection.aspx motion detection].

Before we go into deep discussions about what the application does and how it is implemented, let’s take a look at the very quick [http://www.youtube.com/v/lKkGOUW_xas demo] ...

= Motion detection and object extraction =

Before we can start with hands gesture recognition, first of all we need to extract human’s body, which demonstrates some gesture, and find a good moment, when the actual gesture recognition should be done. For both these tasks we are going to reuse some motion detection ideas described in the dedicated to
[http://www.codeproject.com/KB/audio-video/Motion_Detection.aspx motion detection article].

For object extraction task we are going to use the approach, which is based on background modeling. Let’s suppose that the very first frame of a video stream does not contain any moving objects, but just contains a background scene.

[http://aforge.googlecode.com/svn/wiki/images/hgr/bg.jpg]

Of course such assumption can not be valid for call cases. But, first of all, it may be valid for most of the cases, so it is quite applicable, and the second – our algorithm is going to be adaptive, so it could handle situations, when the first frame contains not only the background. But, let’s be consecutive ... So, our very fist frame can be taken as approximation of background frame.

{{{
// check background frame
if ( backgroundFrame == null )
{
    // save image dimension
    width     = image.Width;
    height    = image.Height;
    frameSize = width * height;

    // create initial backgroung image
    backgroundFrame = grayscaleFilter.Apply( image );

    return;
}
}}}

Now let’s suppose that after a while we receive a new frame, which contains some object, and our task is to extract it.

[http://aforge.googlecode.com/svn/wiki/images/hgr/object.jpg]

When we have two images, the background and the image with an object, we may use _Difference_ filter to get a difference image:

{{{
// apply the grayscale filter
Bitmap currentFrame = grayscaleFilter.Apply( image );

// set backgroud frame as an overlay for difference filter
differenceFilter.OverlayImage = backgroundFrame;

// apply difference filter
Bitmap motionObjectsImage = differenceFilter.Apply( currentFrame );
}}}

[http://aforge.googlecode.com/svn/wiki/images/hgr/dif.jpg]

On the difference image it is possible to see absolute difference between two images – whiter areas show the areas of higher difference and black areas show the areas of no difference. The next two steps are:

*1)* Threshold the difference image using _Threshold_ filter, so each pixel may be classified as significant change (most probably caused by moving object) or as non significant change.

*2)* Remove noise from the thresholded difference image using _Opening_ filter. After this step the stand alone pixels, which could be caused by noisy camera and other circumstances, will be removed, so we’ll have an image, which depicts only more or less significant areas of changes (motion areas).

{{{
// lock motion objects image for further faster processing
BitmapData motionObjectsData = motionObjectsImage.LockBits(
    new Rectangle( 0, 0, width, height ),
    ImageLockMode.ReadWrite, PixelFormat.Format8bppIndexed );

// apply threshold filter
thresholdFilter.ApplyInPlace( motionObjectsData );

// apply opening filter to remove noise
openingFilter.ApplyInPlace( motionObjectsData );
}}}

[http://aforge.googlecode.com/svn/wiki/images/hgr/gesture.jpg]

It looks like we got quite good hands gesture image and we are ready for the next step – recognition ... Not yet. The object’s image we got as an example represents quite recognizable human’s body, which demonstrates us some hands gesture. But, before we get such image in our video stream, we’ll receive a lot of other frames, where we may have many other different objects, which are far from being human body. Such objects could be anything else moving across the scene, or it even could be quite bigger noise than the one we filtered out before. To get rid of some false objects, let’s go through all objects in the image and check their size. To achieve this we are going to use _BlobCounter_ class:

{{{
// process blobs
blobCounter.ProcessImage( motionObjectsData );
Blob[] blobs = blobCounter.GetObjectInformation( );

int maxSize = 0;
Blob maxObject = new Blob( 0, new Rectangle( 0, 0, 0, 0 ) );

// find the biggest blob
if ( blobs != null )
{
    foreach ( Blob blob in blobs )
    {
        int blobSize = blob.Rectangle.Width * blob.Rectangle.Height;

        if ( blobSize > maxSize )
        {
            maxSize = blobSize;
            maxObject = blob;
        }
    }
}
}}}

How are we going to use the information about the biggest object’s size? First of all we are going to implement adaptive background, which we’ve mentioned before. Suppose that from time to time we may have some minor changes in the scene, like minor changes of light condition, some movements of small objects or even a small object has appeared and stayed on the scene. To take these changes into account, we are going to have adaptive background – we are going to change our background frame (which is initially initialized from the first video frame) in the direction of our changes using _MoveTowards+ filter. The _MoveTowards_ filter changes slightly one image in the direction to make smaller difference with the second provided image. For example, if we have a background image, which contains scene only, and an object image, which contains the same scene plus an object on it, then applying sequentially _MoveTowards_ filter to the background image, will make it the same as object image after a while – the more we apply _MoveTowards_ filter to the background image, the more evident becomes the presence of the object on it (the background image becomes "closer" to the object image – the difference becomes smaller).

So, we are checking the size of the biggest object on the current frame and, if it is not that big, we consider the object as not significant and we just update our background frame to adapt to the changes:

{{{
// if we have only small objects then let's adopt to changes in the scene
if ( ( maxObject.Rectangle.Width < 20 ) || ( maxObject.Rectangle.Height < 20 ) )
{
    // move background towards current frame
    moveTowardsFilter.OverlayImage = currentFrame;
    moveTowardsFilter.ApplyInPlace( backgroundFrame );
}
}}}

= Article to be completed ... =